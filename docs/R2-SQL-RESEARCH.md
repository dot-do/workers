# R2 SQL Research - Graph Database Feasibility

**Date:** 2025-10-04
**Status:** Open Beta (Announced ~3 days ago)
**Purpose:** Evaluate R2 SQL for TB-scale MDXLD backlink queries

## What is R2 SQL?

R2 SQL is a **serverless, distributed query engine** for analyzing petabyte-scale data in Cloudflare R2 object storage using Apache Iceberg table format.

### Key Characteristics
- ✅ Petabyte-scale storage
- ✅ Distributed query execution across Cloudflare's global network
- ✅ Apache Iceberg table format
- ✅ Streaming query planning pipeline
- ✅ Intelligent metadata pruning
- ⚠️ Open Beta (very new)
- ⚠️ Limited documentation

## Our Use Case: MDXLD Backlink Queries

### Data Model
**Things** (content):
- `ns` + `id` = URL (e.g., `github.com/dot-do/api`)
- `data` = YAML frontmatter (JSON)
- `content` = Full markdown with YAML

**Relationships** (backlink index):
- `fromNs` + `fromId` → `toNs` + `toId`
- Purpose: Reverse index for "What links TO this page?"
- Auto-generated by parsing markdown

### Primary Query Pattern
```sql
SELECT fromNs, fromId, fromType, predicate, data
FROM relationships
WHERE toNs = 'github.com' AND toId = 'dot-do/api'
ORDER BY fromNs, fromId
LIMIT 1000
```

### R2 SQL Support Analysis
- ✅ Single table SELECT - **SUPPORTED**
- ✅ WHERE clause with = operators - **SUPPORTED**
- ✅ ORDER BY - **SUPPORTED** (with partition key limitation)
- ✅ LIMIT - **SUPPORTED** (1-10,000 range)
- ❌ JOINs - **NOT NEEDED** (single table query)
- ❌ GROUP BY - **NOT NEEDED** (no aggregation)
- ❌ Complex filters - **NOT NEEDED** (simple equality)

**Conclusion: R2 SQL CAN execute our query pattern!**

## SQL Capabilities vs Limitations

### Supported Features
- ✅ Basic SELECT queries (columns, *)
- ✅ Single table FROM
- ✅ WHERE with basic comparison operators (=, !=, <, >, <=, >=)
- ✅ ORDER BY on partition keys
- ✅ LIMIT (1-10,000)
- ✅ Simple boolean logic (AND, OR, NOT)

### NOT Supported (But We Don't Need)
- ❌ Aggregation functions (COUNT, SUM, AVG, etc.)
- ❌ GROUP BY / HAVING
- ❌ JOINs
- ❌ Subqueries
- ❌ Window functions
- ❌ UPD ATE/DELETE operations
- ❌ Array/JSON filtering
- ❌ Column-to-column comparisons

### Critical Limitations
- ⚠️ ORDER BY only works on partition keys
- ⚠️ No schema evolution support
- ⚠️ No column aliasing
- ⚠️ LIMIT range: 1-10,000 only

## Setup Process

### 1. Create R2 Bucket
```bash
npx wrangler r2 bucket create mdxld-graph
```

### 2. Enable Data Catalog
```bash
npx wrangler r2 bucket catalog enable mdxld-graph
```
**Returns:**
- Warehouse name
- Catalog URI

### 3. Create API Token
- Dashboard: Create API token with Admin Read & Write permissions
- Save token for authentication

### 4. Data Ingestion Options

**Option A: Pipelines (Recommended by Cloudflare)**
- Create pipeline with HTTP endpoint
- Define schema (JSON)
- Send data via HTTP POST
- Automatic Iceberg table management

**Option B: Direct Iceberg Write (Unknown feasibility)**
- Write Parquet files directly to R2
- Update Iceberg metadata
- More control, more complexity

### 5. Query via Wrangler
```bash
export WRANGLER_R2_SQL_AUTH_TOKEN="your-api-token"
npx wrangler r2 sql query "warehouse-name" "SELECT * FROM default.relationships LIMIT 10"
```

## Performance Considerations

### Unknown (No Public Benchmarks)
- ❓ Query latency for simple WHERE clauses
- ❓ Performance with TB-scale datasets
- ❓ Cold start vs warm cache performance
- ❓ Partition key optimization impact
- ❓ Cost at scale

### Architectural Advantages
- ✅ Distributed execution across global network
- ✅ Streaming query planning (low latency start)
- ✅ Multi-layer metadata pruning
- ✅ Vectorized execution (Apache DataFusion)
- ✅ Column-selective reads (Parquet)

### Potential Issues
- ⚠️ Partition key required for ORDER BY
- ⚠️ Open beta stability
- ⚠️ Limited documentation
- ⚠️ No direct Worker API (must use Wrangler or HTTP)

## Comparison: R2 SQL vs ClickHouse

| Feature | R2 SQL | Workers Analytics Engine (ClickHouse) |
|---------|--------|---------------------------------------|
| **Status** | Open Beta | Production Ready |
| **Scale** | Petabytes | Billions of rows |
| **Query Latency** | Unknown | Sub-10ms (documented) |
| **SQL Support** | Limited (no JOINs, aggregations) | Full SQL |
| **Our Query** | ✅ Supported | ✅ Supported |
| **Partition Keys** | Required for ORDER BY | Optional |
| **Documentation** | Minimal | Extensive |
| **Cost** | Free during beta | $1/million queries |
| **Integration** | Wrangler CLI | HTTP API + Worker fetch |

## Critical Questions to Answer

1. **Query Latency**: Can R2 SQL achieve <100ms for backlink queries?
2. **Partition Strategy**: What's the optimal partition key for `toNs + toId` lookups?
3. **Write Performance**: How fast can we ingest relationships?
4. **Scale Behavior**: Does latency degrade with TB-scale data?
5. **Cost**: What will pricing be after beta?

## Next Steps

### Phase 1: Setup & Basic Test
1. Create R2 bucket + enable catalog
2. Set up pipeline for relationships table
3. Import 50 sample relationships (ONET data)
4. Test basic query: "What requires JavaScript?"
5. Measure latency (1st query, 10th query, 100th query)

### Phase 2: Partition Optimization
1. Test partition strategies:
   - Partition by `toNs`
   - Partition by `toNs + toId` (if supported)
   - No partitioning (baseline)
2. Measure query latency difference
3. Document findings

### Phase 3: Scale Testing
1. Import 5K relationships
2. Re-measure latency
3. Import 50K relationships
4. Re-measure latency
5. Identify degradation patterns

### Phase 4: Comparison
1. Run identical queries on D1
2. Mock ClickHouse queries (or set up if feasible)
3. Create comparison matrix
4. Make recommendation

## Recommended Partition Strategy

For our use case (`WHERE toNs = X AND toId = Y`):

**Optimal Partition Key**: `toNs, toId`

**Reasoning**:
- Queries filter on both `toNs` and `toId`
- Iceberg metadata can prune files by partition
- Reduces data scanned per query
- Aligns with Apache Iceberg best practices

**Schema**:
```json
{
  "fields": [
    { "name": "fromNs", "type": "string" },
    { "name": "fromId", "type": "string" },
    { "name": "fromType", "type": "string" },
    { "name": "predicate", "type": "string" },
    { "name": "toNs", "type": "string" },
    { "name": "toId", "type": "string" },
    { "name": "toType", "type": "string" },
    { "name": "data", "type": "string" },
    { "name": "createdAt", "type": "timestamp" }
  ],
  "partitionBy": ["toNs", "toId"]
}
```

## Open Questions

1. Can we partition on multiple columns in R2 SQL/Iceberg?
2. Is there a Worker-native API or only Wrangler CLI?
3. Can we use Pipelines API from Workers?
4. What's the batch write performance?
5. Is there query caching or connection pooling?

## References

- **R2 SQL Docs**: https://developers.cloudflare.com/r2-sql/
- **Deep Dive Blog**: https://blog.cloudflare.com/r2-sql-deep-dive/
- **Data Platform Announcement**: https://blog.cloudflare.com/cloudflare-data-platform/
- **Limitations**: https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/
- **Getting Started**: https://developers.cloudflare.com/r2-sql/get-started/

## Status

**Current Phase**: Research Complete
**Next**: Setup R2 bucket and run first query test
**Timeline**: 1-2 hours for basic setup and initial benchmark

---

**Last Updated**: 2025-10-04
**Researcher**: Claude Code
**Confidence**: Medium (limited documentation, very new product)
